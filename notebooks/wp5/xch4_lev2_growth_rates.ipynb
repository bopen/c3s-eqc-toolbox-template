{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# xch4 level 2 growth rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import download\n",
    "from xarray.groupers import BinGrouper\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = 2004\n",
    "year_stop = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"satellite-methane\"\n",
    "request = {\n",
    "    \"processing_level\": \"level_2\",\n",
    "    \"variable\": \"xch4\",\n",
    "    \"sensor_and_algorithm\": \"merged_emma\",\n",
    "    \"version\": \"4_5\",\n",
    "    \"year\": [str(year) for year in range(year_start - 1, year_stop + 2)],\n",
    "    \"month\": [f\"{i:02d}\" for i in range(1, 13)],\n",
    "    \"day\": [f\"{i:02d}\" for i in range(1, 32)],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Define functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_dataset(obj):\n",
    "    weights = np.abs(np.cos(np.deg2rad(obj[\"latitude\"])))\n",
    "    return obj.weighted(weights)\n",
    "\n",
    "\n",
    "def spatial_weighted_mean(obj, dim=None):\n",
    "    return weight_dataset(obj).mean(dim, keep_attrs=True)\n",
    "\n",
    "\n",
    "def spatial_weighted_std(obj, dim=None):\n",
    "    return weight_dataset(obj).std(dim, keep_attrs=True)\n",
    "\n",
    "\n",
    "def regrid(ds, d_lon, d_lat, lon1=180):\n",
    "    if lon1 not in (180, 360):\n",
    "        raise ValueError(f\"lon1 must be 180 or 360. {lon1=}\")\n",
    "    lon0 = -180 if lon1 == 180 else 0\n",
    "\n",
    "    coords = {}\n",
    "    for name, start, stop, step in zip(\n",
    "        [\"latitude\", \"longitude\"],\n",
    "        [-90, lon0],\n",
    "        [90, lon1],\n",
    "        [d_lat, d_lon],\n",
    "    ):\n",
    "        if step is None:\n",
    "            continue\n",
    "        coords[name] = BinGrouper(\n",
    "            np.arange(start, stop + step, step),\n",
    "            include_lowest=True,\n",
    "            labels=np.arange(start + step / 2, stop + step / 2, step),\n",
    "        )\n",
    "    ds = ds.compute()  # Groupby map does not work with dask\n",
    "    ds = ds.groupby(**coords).map(spatial_weighted_mean)\n",
    "    ds = ds.drop_vars(set(coords) & set(ds.variables)).rename(\n",
    "        {f\"{coord}_bins\": coord for coord in coords}\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def daily_regrid(ds):\n",
    "    ds = ds[[\"xch4\", \"latitude\", \"longitude\"]]\n",
    "    datasets = []\n",
    "    for time, ds_time in tqdm.tqdm(ds.resample(time=\"1D\")):\n",
    "        ds_time = regrid(ds_time, d_lon=2, d_lat=2)\n",
    "        datasets.append(ds_time.expand_dims(time=[time]))\n",
    "    return xr.concat(datasets, \"time\")\n",
    "\n",
    "\n",
    "def monthly_regrid_in_bands(ds, zonal_first):\n",
    "    datasets = []\n",
    "    for time, ds_time in ds.resample(time=\"1MS\"):\n",
    "        if zonal_first:\n",
    "            ds_time = ds_time.mean(\"longitude\", keep_attrs=True)\n",
    "        ds_time = regrid(ds_time, d_lat=20, d_lon=None)\n",
    "        datasets.append(ds_time.expand_dims(time=[time]))\n",
    "    return xr.concat(datasets, \"time\")\n",
    "\n",
    "\n",
    "def compute_growth_rate(ds, zonal_first):\n",
    "    da = monthly_regrid_in_bands(ds, zonal_first)[\"xch4\"]\n",
    "    da = (\n",
    "        da.rolling(time=12, center=True)\n",
    "        .construct(\"window_dim\")\n",
    "        .isel(window_dim=[0, -1])\n",
    "        .diff(\"window_dim\")\n",
    "        .squeeze()\n",
    "    )\n",
    "    da.attrs = {\"units\": \"ppm/year\", \"long_name\": \"Growth Rate\"}\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Download and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = download.download_and_transform(\n",
    "    collection_id,\n",
    "    request,\n",
    "    chunks={\"year\": 1},\n",
    "    transform_func=daily_regrid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Compute growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataarrays = []\n",
    "for zonal_first in [True, False]:\n",
    "    da = compute_growth_rate(ds, zonal_first=zonal_first)\n",
    "    dataarrays.append(\n",
    "        da.expand_dims(method=[\"Zonal-first\" if zonal_first else \"Standard\"])\n",
    "    )\n",
    "da = xr.concat(dataarrays, \"method\").sel(time=slice(str(year_start), str(year_stop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Plot monthly growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "facet = da.plot(col=\"latitude\", col_wrap=3, hue=\"method\")\n",
    "for ax in facet.axs.flatten():\n",
    "    ax.grid()\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(keep_attrs=True):\n",
    "    anomaly = da - spatial_weighted_mean(da)\n",
    "anomaly.attrs[\"long_name\"] = \"Î”\" + anomaly.long_name\n",
    "facet = anomaly.plot(row=\"method\", robust=True, x=\"time\", figsize=(10, 10))\n",
    "for ax in facet.axs.flatten():\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Plot yearly mean growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = da.groupby(\"time.year\")\n",
    "da_mean = grouped.map(spatial_weighted_mean, dim=[\"time\", \"latitude\"])\n",
    "da_std = grouped.map(spatial_weighted_std, dim=[\"time\", \"latitude\"])\n",
    "df_mean = da_mean.to_pandas().T\n",
    "df_std = da_std.to_pandas().T\n",
    "ax = df_mean.plot.bar(yerr=df_std)\n",
    "ax.grid()\n",
    "_ = ax.set_ylabel(f\"{da_mean.long_name} [{da_mean.units}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
