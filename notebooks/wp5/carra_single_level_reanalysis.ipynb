{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CARRA Single Level Reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import diagnostics, download, plot\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Time\n",
    "start = \"1991-01\"\n",
    "stop = \"2020-12\"\n",
    "\n",
    "# Region\n",
    "domain = \"west_domain\"\n",
    "assert domain in (\"east_domain\", \"west_domain\")\n",
    "\n",
    "# Product type\n",
    "product_type = \"forecast\"\n",
    "assert product_type in (\"analysis\", \"forecast\")\n",
    "\n",
    "# Variable\n",
    "variable = \"total_precipitation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"reanalysis-carra-single-levels\"\n",
    "request = {\n",
    "    \"domain\": domain,\n",
    "    \"level_type\": \"surface_or_atmosphere\",\n",
    "    \"variable\": variable,\n",
    "    \"product_type\": product_type,\n",
    "    \"time\": \"12:00\",\n",
    "}\n",
    "if product_type == \"forecast\":\n",
    "    request[\"leadtime_hour\"] = \"1\"\n",
    "requests = download.update_request_date(\n",
    "    request, start=start, stop=stop, stringify_dates=True\n",
    ")\n",
    "chunks = {\"year\": 1, \"month\": 1}\n",
    "\n",
    "# Parameters to speed up I/O\n",
    "open_mfdataset_kwargs = {\n",
    "    \"concat_dim\": \"forecast_reference_time\",\n",
    "    \"combine\": \"nested\",\n",
    "    \"data_vars\": \"minimal\",\n",
    "    \"coords\": \"minimal\",\n",
    "    \"compat\": \"override\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_da(ds):\n",
    "    (varname,) = ds.data_vars\n",
    "    return ds[varname]\n",
    "\n",
    "\n",
    "def rechunk(da, tmpdir=None):\n",
    "    if tmpdir:\n",
    "        # Auto-chunking with dask\n",
    "        chunks = dict(\n",
    "            da.chunk(\n",
    "                {\n",
    "                    dim: -1 if dim == \"forecast_reference_time\" else \"auto\"\n",
    "                    for dim in da.dims\n",
    "                }\n",
    "            )\n",
    "            .unify_chunks()\n",
    "            .chunksizes\n",
    "        )\n",
    "\n",
    "        dataarrays = []\n",
    "        for year, da in da.groupby(\"forecast_reference_time.year\"):\n",
    "            # Split in yearly files\n",
    "            da = da.chunk(chunks | {\"forecast_reference_time\": -1}).unify_chunks()\n",
    "            da.to_zarr(f\"{tmpdir}/{year}.zarr\")\n",
    "            ds = xr.open_dataset(\n",
    "                f\"{tmpdir}/{year}.zarr\", engine=\"zarr\", chunks=dict(da.chunksizes)\n",
    "            )\n",
    "            da = ds.set_coords(da.coords)[da.name]\n",
    "            dataarrays.append(da)\n",
    "        da = xr.concat(dataarrays, \"forecast_reference_time\")\n",
    "\n",
    "    # chunk of size 1 for additional dimensions\n",
    "    da = da.chunk(\n",
    "        **{\n",
    "            dim: 1\n",
    "            for dim in da.dims\n",
    "            if dim not in (\"forecast_reference_time\", \"x\", \"y\")\n",
    "        }\n",
    "    )\n",
    "    da.encoding[\"chunksizes\"] = tuple(map(max, da.chunks))\n",
    "    return da\n",
    "\n",
    "\n",
    "def rechunk_and_reduce(da, reduce_func, **kwargs):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        return reduce_func(rechunk(da, tmpdir), **kwargs).compute()\n",
    "\n",
    "\n",
    "def compute_time_weighted_reduction(ds, group, func, **kwargs):\n",
    "    da = get_da(ds)\n",
    "    if group:\n",
    "        da = da.groupby(group).map(rechunk_and_reduce, reduce_func=func, **kwargs)\n",
    "    else:\n",
    "        da = rechunk_and_reduce(da, reduce_func=func, **kwargs)\n",
    "    return rechunk(da).to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Compute time reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_datasets = {}\n",
    "for group in (None, \"forecast_reference_time.month\"):\n",
    "    dataarrays = []\n",
    "    for func in (\n",
    "        diagnostics.time_weighted_mean,\n",
    "        diagnostics.time_weighted_std,\n",
    "        diagnostics.time_weighted_linear_trend,\n",
    "    ):\n",
    "        print(f\"{group=} {func.__name__=}\")\n",
    "        ds = download.download_and_transform(\n",
    "            collection_id,\n",
    "            requests,\n",
    "            transform_func=compute_time_weighted_reduction,\n",
    "            transform_func_kwargs={\"group\": group, \"func\": func, \"weights\": False},\n",
    "            transform_chunks=False,\n",
    "            chunks=chunks,\n",
    "            **open_mfdataset_kwargs,\n",
    "        )\n",
    "        da = rechunk(get_da(ds)).rename(func.__name__)\n",
    "\n",
    "        # Convert and set attributes\n",
    "        name = func.__name__.replace(\"time_weighted_\", \"\")\n",
    "        attrs = {\"long_name\": f\"{name} of {variable}\".replace(\"_\", \" \").capitalize()}\n",
    "        if func == diagnostics.time_weighted_linear_trend:\n",
    "            with xr.set_options(keep_attrs=True):\n",
    "                da *= 60 * 60 * 24 * 365\n",
    "            attrs[\"units\"] = da.attrs[\"units\"].replace(\"s-1\", \"year-1\")\n",
    "        else:\n",
    "            attrs[\"units\"] = da.attrs[\"units\"]\n",
    "        da.attrs = attrs\n",
    "        dataarrays.append(da)\n",
    "    maps_datasets[group] = xr.merge(dataarrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Compute spatial weighted reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if variable == \"total_precipitation\":\n",
    "    transform_funcs = (diagnostics.spatial_weighted_quantile,)\n",
    "    transform_func_kwargs = {\"q\": (1 / 3, 1 / 2, 2 / 3)}\n",
    "else:\n",
    "    transform_funcs = (\n",
    "        diagnostics.spatial_weighted_mean,\n",
    "        diagnostics.spatial_weighted_std,\n",
    "    )\n",
    "    transform_func_kwargs = {}\n",
    "\n",
    "dataarrays = []\n",
    "for transform_func in transform_funcs:\n",
    "    print(f\"{transform_func.__name__=}\")\n",
    "    ds = download.download_and_transform(\n",
    "        collection_id,\n",
    "        requests,\n",
    "        transform_chunks=True,\n",
    "        transform_func=transform_func,\n",
    "        transform_func_kwargs=transform_func_kwargs,\n",
    "        chunks=chunks,\n",
    "        **open_mfdataset_kwargs,\n",
    "    )\n",
    "    dataarrays.append(get_da(ds).rename(transform_func.__name__))\n",
    "ds_timeseries = xr.merge(dataarrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Plot maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in maps_datasets.values():\n",
    "    (col,) = (set(ds.dims) - {\"forecast_reference_time\", \"x\", \"y\"}) or {None}\n",
    "    projection = ccrs.LambertConformal(\n",
    "        central_longitude=ds[\"longitude\"].mean().values,\n",
    "        central_latitude=ds[\"latitude\"].mean().values,\n",
    "    )\n",
    "    for var, da in ds.data_vars.items():\n",
    "        plot_obj = plot.projected_map(\n",
    "            da,\n",
    "            projection=projection,\n",
    "            col=col,\n",
    "            col_wrap=3,\n",
    "        )\n",
    "        gridliners = (\n",
    "            [gl for ax in plot_obj.axs.flat for gl in ax._gridliners]\n",
    "            if col\n",
    "            else plot_obj.axes._gridliners\n",
    "        )\n",
    "        for gl in gridliners:\n",
    "            gl.x_inline = False\n",
    "            gl.xlabel_style = {\"rotation\": -45}\n",
    "        title = f\"{collection_id.replace('-', ' ')}\\nFrom {start} to {stop}\".title()\n",
    "        plt.suptitle(title, y=1, va=\"bottom\") if col else plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Plot timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "if \"quantile\" in ds_timeseries.dims:\n",
    "    da = ds_timeseries[\"spatial_weighted_quantile\"]\n",
    "    da.sel(quantile=1 / 2).plot(ax=ax, label=\"median\")\n",
    "    ax.fill_between(\n",
    "        da[\"time\"],\n",
    "        da.sel(quantile=1 / 3),\n",
    "        da.sel(quantile=2 / 3),\n",
    "        alpha=0.5,\n",
    "        label=\"tertiles\",\n",
    "    )\n",
    "else:\n",
    "    ds_timeseries[\"spatial_weighted_mean\"].plot(ax=ax, label=\"mean\")\n",
    "    ax.fill_between(\n",
    "        ds_timeseries[\"time\"],\n",
    "        ds_timeseries[\"spatial_weighted_mean\"] - ds_timeseries[\"spatial_weighted_std\"],\n",
    "        ds_timeseries[\"spatial_weighted_mean\"] + ds_timeseries[\"spatial_weighted_std\"],\n",
    "        alpha=0.5,\n",
    "        label=\"mean ± std\",\n",
    "    )\n",
    "ax.grid()\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1, 1))\n",
    "_ = ax.set_title(\n",
    "    f\"{collection_id}\\n{domain}\".replace(\"-\", \" \").replace(\"_\", \" \").title()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
