{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 0.002238,
     "end_time": "2025-10-16T08:43:32.589080",
     "exception": false,
     "start_time": "2025-10-16T08:43:32.586842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Steric Sea Level Contribution from ORAS5 Reanalysis to Global Sea Level Observed by Satellite Altimetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 0.00161,
     "end_time": "2025-10-16T08:43:32.592663",
     "exception": false,
     "start_time": "2025-10-16T08:43:32.591053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 32.032443,
     "end_time": "2025-10-16T08:44:04.626674",
     "exception": false,
     "start_time": "2025-10-16T08:43:32.594231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cacholote\n",
    "import gsw_xarray as gsw\n",
    "import matplotlib.pyplot as plt\n",
    "import pooch\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import diagnostics, download, plot, utils\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.001786,
     "end_time": "2025-10-16T08:44:14.153207",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.151421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.0054,
     "end_time": "2025-10-16T08:44:14.160890",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.155490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Time\n",
    "year_start = 2004\n",
    "year_stop = 2023\n",
    "\n",
    "# Space\n",
    "lat_slice = slice(-60, 60)\n",
    "lon_slice = slice(-180, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.001968,
     "end_time": "2025-10-16T08:44:14.164952",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.162984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2025-10-16T08:44:14.173759",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.166748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_request = {\n",
    "    \"year\": [str(year) for year in range(year_start, year_stop + 1)],\n",
    "    \"month\": [f\"{month:02d}\" for month in range(1, 13)],\n",
    "}\n",
    "\n",
    "request_reanalysis = (\n",
    "    \"reanalysis-oras5\",\n",
    "    {\n",
    "        \"product_type\": [\"consolidated\", \"operational\"],\n",
    "        \"vertical_resolution\": \"all_levels\",\n",
    "        \"variable\": [\"potential_temperature\", \"salinity\"],\n",
    "    }\n",
    "    | time_request,\n",
    ")\n",
    "request_satellite = (\n",
    "    \"satellite-sea-level-global\",\n",
    "    {\"variable\": [\"monthly_mean\"], \"version\": \"vdt2024\"} | time_request,\n",
    ")\n",
    "\n",
    "download_kwargs = {\n",
    "    \"chunks\": {\"year\": 1, \"month\": 1},\n",
    "    \"drop_variables\": [\"time_counter_bnds\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.001963,
     "end_time": "2025-10-16T08:44:14.177755",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.175792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "papermill": {
     "duration": 0.009396,
     "end_time": "2025-10-16T08:44:14.188917",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.179521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gsw_ds(ds):\n",
    "    p = gsw.p_from_z(-ds[\"deptht\"], ds[\"latitude\"])\n",
    "    SA = gsw.SA_from_SP(ds[\"vosaline\"], p, ds[\"longitude\"], ds[\"latitude\"])\n",
    "    if \"votemper\" in ds.data_vars:\n",
    "        CT = gsw.CT_from_pt(SA, ds[\"votemper\"])\n",
    "    else:\n",
    "        CT = gsw.CT_from_t(SA, ds[\"temperature\"], p)\n",
    "    rho = gsw.rho(SA, CT, p)\n",
    "    ds = xr.merge([p, SA, CT, rho])\n",
    "    for da in ds.data_vars.values():\n",
    "        chunks = []\n",
    "        for dim in da.dims:\n",
    "            if dim == \"time\":\n",
    "                chunks.append(1)\n",
    "            elif dim in [\"x\", \"y\", \"latitude\", \"longitude\"]:\n",
    "                chunks.append(200)\n",
    "            elif dim == \"deptht\":\n",
    "                chunks.append(15)\n",
    "            else:\n",
    "                raise ValueError(f\"{dim = }\")\n",
    "        da.encoding[\"chunksizes\"] = tuple(chunks)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def compute_ssl_from_rho(rho, rho0=1025, prefix=\"\"):\n",
    "    grouped_rho = rho.groupby(\"time.month\")\n",
    "    delta_rho = grouped_rho - grouped_rho.mean()\n",
    "    ssl = -(delta_rho.fillna(0) / rho0).integrate(\"deptht\")\n",
    "    ssl.attrs = {\"long_name\": f\"{prefix}steric sea level\".title(), \"units\": \"m\"}\n",
    "    return ssl\n",
    "\n",
    "\n",
    "def compute_ssl_from_ds(ds, prefix, lon_slice, lat_slice):\n",
    "    ds = utils.regionalise(ds, lon_slice=lon_slice, lat_slice=lat_slice)\n",
    "    if prefix == \"\":\n",
    "        rho = ds[\"rho\"]\n",
    "    elif prefix == \"thermo\":\n",
    "        rho = gsw.rho(ds[\"SA\"].mean(\"time\"), ds[\"CT\"], ds[\"p\"])\n",
    "    elif prefix == \"halo\":\n",
    "        rho = gsw.rho(ds[\"SA\"], ds[\"CT\"].mean(\"time\"), ds[\"p\"])\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{prefix=}\")\n",
    "    return compute_ssl_from_rho(rho, prefix=prefix).rename(f\"{prefix}ssl\")\n",
    "\n",
    "\n",
    "def compute_ssl_timeseries_from_ds(ds, prefix, lon_slice, lat_slice):\n",
    "    ssl = compute_ssl_from_ds(ds, prefix, lon_slice, lat_slice)\n",
    "    return diagnostics.spatial_weighted_mean(ssl)\n",
    "\n",
    "\n",
    "def compute_ssl_trend_from_ds(ds, prefix, lon_slice, lat_slice):\n",
    "    ssl = compute_ssl_from_ds(ds, prefix, lon_slice, lat_slice)\n",
    "    coords = ssl.to_dataset().drop_dims(\"time\").coords\n",
    "    ssl = diagnostics.time_weighted_linear_trend(ssl)\n",
    "    return ssl.assign_coords(coords)\n",
    "\n",
    "\n",
    "@cacholote.cacheable\n",
    "def compute_ssl_timeseries(\n",
    "    collection_id,\n",
    "    request,\n",
    "    prefix,\n",
    "    lon_slice,\n",
    "    lat_slice,\n",
    "    **download_kwargs,\n",
    "):\n",
    "    ds = download.download_and_transform(\n",
    "        collection_id, request, transform_func=compute_gsw_ds, **download_kwargs\n",
    "    )\n",
    "    return compute_ssl_timeseries_from_ds(\n",
    "        ds, prefix, lon_slice=lon_slice, lat_slice=lat_slice\n",
    "    )\n",
    "\n",
    "\n",
    "@cacholote.cacheable\n",
    "def compute_ssl_trend(\n",
    "    collection_id,\n",
    "    request,\n",
    "    prefix,\n",
    "    lon_slice,\n",
    "    lat_slice,\n",
    "    **download_kwargs,\n",
    "):\n",
    "    ds = download.download_and_transform(\n",
    "        collection_id, request, transform_func=compute_gsw_ds, **download_kwargs\n",
    "    )\n",
    "    return compute_ssl_trend_from_ds(\n",
    "        ds, prefix, lon_slice=lon_slice, lat_slice=lat_slice\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(ds):\n",
    "    # Naming\n",
    "    ds = ds.rename({var: var.lower() for var in ds.variables})\n",
    "    # Time\n",
    "    ds[\"time\"].attrs[\"calendar\"] = \"360_day\"\n",
    "    ds = xr.decode_cf(ds)\n",
    "    ds[\"time\"].attrs = {\"standard_name\": \"time\"}\n",
    "    ds[\"time\"].encoding = {}\n",
    "    # Depth\n",
    "    ds[\"depth\"] = -gsw.z_from_p(ds[\"pressure\"], ds[\"latitude\"]).mean(\n",
    "        \"latitude\", keep_attrs=True\n",
    "    )\n",
    "    ds[\"depth\"].attrs.update({\"positive\": \"down\", \"long_name\": \"Depth from pressure\"})\n",
    "    return ds.swap_dims(pressure=\"depth\")\n",
    "\n",
    "\n",
    "@cacholote.cacheable\n",
    "def get_argo(year, month):\n",
    "    # Get climatology\n",
    "    filenames = []\n",
    "    for var in [\"Temperature\", \"Salinity\"]:\n",
    "        url = f\"https://sio-argo.ucsd.edu/RG/RG_ArgoClim_{var}_2019.nc.gz\"\n",
    "        filename = pooch.retrieve(\n",
    "            url=url, known_hash=None, processor=pooch.Decompress()\n",
    "        )\n",
    "        filenames.append(filename)\n",
    "    ds = xr.open_mfdataset(filenames, preprocess=preprocess, decode_times=False)\n",
    "    ds_clima = ds.drop_dims(\"time\")\n",
    "\n",
    "    # Get anomalies\n",
    "    ds = ds.sel(time=slice(f\"{year}-{month:02d}\", f\"{year}-{month:02d}\"))\n",
    "    if not ds.sizes[\"time\"]:\n",
    "        url = f\"https://sio-argo.ucsd.edu/RG/RG_ArgoClim_{year}{month:02d}_2019.nc.gz\"\n",
    "        filename = pooch.retrieve(\n",
    "            url=url, known_hash=None, processor=pooch.Decompress()\n",
    "        )\n",
    "        ds = xr.open_mfdataset(filename, preprocess=preprocess, decode_times=False)\n",
    "        ds = ds.sel(time=slice(f\"{year}-{month:02d}\", f\"{year}-{month:02d}\"))\n",
    "\n",
    "    # Compute values\n",
    "    dataarrays = []\n",
    "    for var in [\"salinity\", \"temperature\"]:\n",
    "        da = ds_clima[f\"argo_{var}_mean\"] + ds[f\"argo_{var}_anomaly\"]\n",
    "        dataarrays.append(da.rename(var))\n",
    "    ds = xr.merge(dataarrays)\n",
    "\n",
    "    # Compute gsw dataset\n",
    "    ds = ds.rename(depth=\"deptht\", salinity=\"vosaline\")\n",
    "    ds = compute_gsw_ds(ds)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def get_all_argo(year_start, year_stop):\n",
    "    datasets = []\n",
    "    for year in range(year_start, year_stop + 1):\n",
    "        for month in range(1, 13):\n",
    "            datasets.append(get_argo(year, month))\n",
    "    ds = xr.concat(datasets, \"time\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "@cacholote.cacheable\n",
    "def compute_ssl_timeseries_argo(prefix, lon_slice, lat_slice, year_start, year_stop):\n",
    "    ds = get_all_argo(year_start, year_stop)\n",
    "    return compute_ssl_timeseries_from_ds(ds, prefix, lon_slice, lat_slice)\n",
    "\n",
    "\n",
    "@cacholote.cacheable\n",
    "def compute_ssl_trend_argo(prefix, lon_slice, lat_slice, year_start, year_stop):\n",
    "    ds = get_all_argo(year_start, year_stop)\n",
    "    return compute_ssl_trend_from_ds(ds, prefix, lon_slice, lat_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 0.001777,
     "end_time": "2025-10-16T08:44:14.193127",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.191350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download and transform reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 6.47401,
     "end_time": "2025-10-16T08:44:20.668917",
     "exception": false,
     "start_time": "2025-10-16T08:44:14.194907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefixes = [\"\", \"thermo\", \"halo\"]\n",
    "\n",
    "timeseries = []\n",
    "trends = []\n",
    "for prefix in prefixes:\n",
    "    name = \"_\".join(([prefix] if prefix else []) + [\"ssl\"])\n",
    "    print(f\"{name = }\")\n",
    "    da = compute_ssl_timeseries(\n",
    "        *request_reanalysis,\n",
    "        prefix=prefix,\n",
    "        lon_slice=lon_slice,\n",
    "        lat_slice=lat_slice,\n",
    "        **download_kwargs,\n",
    "    )\n",
    "    timeseries.append(da.rename(name))\n",
    "\n",
    "    da = compute_ssl_trend(\n",
    "        *request_reanalysis,\n",
    "        prefix=prefix,\n",
    "        lon_slice=lon_slice,\n",
    "        lat_slice=lat_slice,\n",
    "        **download_kwargs,\n",
    "    )\n",
    "    if \"latitude\" not in da.coords:\n",
    "        print(\"Delete!\")\n",
    "        cacholote.delete(\n",
    "            compute_ssl_trend,\n",
    "            *request_reanalysis,\n",
    "            prefix=prefix,\n",
    "            lon_slice=lon_slice,\n",
    "            lat_slice=lat_slice,\n",
    "            **download_kwargs,\n",
    "        )\n",
    "        da = compute_ssl_trend(\n",
    "            *request_reanalysis,\n",
    "            prefix=prefix,\n",
    "            lon_slice=lon_slice,\n",
    "            lat_slice=lat_slice,\n",
    "            **download_kwargs,\n",
    "        )\n",
    "    trends.append(da.rename(name))\n",
    "ds_reanalysis_timeseries = xr.merge(timeseries)\n",
    "ds_reanalysis_trend = xr.merge(trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Download and transform ARGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"\", \"thermo\", \"halo\"]\n",
    "\n",
    "timeseries = []\n",
    "trends = []\n",
    "for prefix in prefixes:\n",
    "    name = \"_\".join(([prefix] if prefix else []) + [\"ssl\"])\n",
    "    print(f\"{name = }\")\n",
    "    da = compute_ssl_timeseries_argo(\n",
    "        prefix=prefix,\n",
    "        lon_slice=lon_slice,\n",
    "        lat_slice=lat_slice,\n",
    "        year_start=year_start,\n",
    "        year_stop=year_stop,\n",
    "    )\n",
    "    timeseries.append(da.rename(name))\n",
    "    da = compute_ssl_trend_argo(\n",
    "        prefix=prefix,\n",
    "        lon_slice=lon_slice,\n",
    "        lat_slice=lat_slice,\n",
    "        year_start=year_start,\n",
    "        year_stop=year_stop,\n",
    "    )\n",
    "    trends.append(da.rename(name))\n",
    "ds_argo_timeseries = xr.merge(timeseries)\n",
    "ds_argo_trend = xr.merge(trends)\n",
    "\n",
    "# Align\n",
    "ds_argo_timeseries[\"time\"] = ds_argo_timeseries[\"time\"].convert_calendar(\n",
    "    \"proleptic_gregorian\", align_on=\"date\"\n",
    ")\n",
    "if (ds_argo_timeseries[\"time\"] == ds_reanalysis_timeseries[\"time\"]).all():\n",
    "    ds_argo_timeseries[\"time\"] = ds_reanalysis_timeseries[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Plot timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_timeseries = xr.concat(\n",
    "    [\n",
    "        ds_reanalysis_timeseries.expand_dims(product=[\"ORAS5\"]),\n",
    "        ds_argo_timeseries.expand_dims(product=[\"ARGO\"]),\n",
    "    ],\n",
    "    \"product\",\n",
    ")\n",
    "da = ds_timeseries.to_dataarray()\n",
    "facet = da.plot(hue=\"variable\", col=\"product\")\n",
    "for ax in facet.axs.flatten():\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Plot maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, ds in zip([\"ORAS5\", \"ARGO\"], [ds_reanalysis_trend, ds_argo_trend]):\n",
    "    da = ds.to_dataarray()\n",
    "    facet = plot.projected_map(da, col=\"variable\", robust=True)\n",
    "    for ax in facet.axs.flatten():\n",
    "        ax.set_extent(\n",
    "            [lon_slice.start, lon_slice.stop, lat_slice.start, lat_slice.stop]\n",
    "        )\n",
    "    facet.fig.suptitle(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Download and transform satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_satellite = download.download_and_transform(*request_satellite, **download_kwargs)\n",
    "ds_satellite = utils.regionalise(ds_satellite, lon_slice=lon_slice, lat_slice=lat_slice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 95.402466,
   "end_time": "2025-10-16T08:45:01.588822",
   "environment_variables": {},
   "exception": null,
   "input_path": "steric_sea_level_ora5.ipynb",
   "output_path": "output.ipynb",
   "parameters": {},
   "start_time": "2025-10-16T08:43:26.186356",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
