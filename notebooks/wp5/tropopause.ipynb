{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tropopause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import cdsapi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import statsmodels.tsa.seasonal\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import download\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period\n",
    "start = \"2006-05\"\n",
    "stop = \"2020-03\"\n",
    "\n",
    "# Stations\n",
    "stations = [\"TEN\", \"LIN\", \"NYA\"]  # Use None to analyse all stations\n",
    "assert isinstance(stations, list | None)\n",
    "\n",
    "# Directory for csv files\n",
    "csv_dir = \"./csv_files\"\n",
    "\n",
    "# CDS credentials\n",
    "os.environ[\"CDSAPI_RC\"] = os.path.expanduser(\"~/ciardini_virginia/.cdsapirc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"insitu-observations-gruan-reference-network\"\n",
    "request = {\n",
    "    \"variable\": [\"air_temperature\", \"relative_humidity\", \"air_pressure\", \"altitude\"],\n",
    "    \"format\": \"netcdf\",\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "requests = []\n",
    "for date in pd.date_range(start, stop, freq=\"1MS\"):\n",
    "    time_request = {\"year\": date.strftime(\"%Y\"), \"month\": date.strftime(\"%m\")}\n",
    "    time_request[\"day\"] = client.client.apply_constraints(\n",
    "        collection_id, request | time_request\n",
    "    )[\"day\"]\n",
    "    if time_request[\"day\"]:\n",
    "        requests.append(request | time_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reorganize_dataset(ds):\n",
    "    # Rename\n",
    "    (varname,) = set(ds[\"observed_variable\"].values)\n",
    "    ds = ds.rename(observation_value=str(varname)).drop_vars(\"observed_variable\")\n",
    "    ds = ds.rename(\n",
    "        {\n",
    "            var: \"_\".join([varname, var.replace(\"_value\", \"\")])\n",
    "            for var in ds.data_vars\n",
    "            if var.startswith(\"uncertainty\")\n",
    "        }\n",
    "    )\n",
    "    # Update attrs\n",
    "    for var, da in ds.data_vars.items():\n",
    "        match var:\n",
    "            case \"pressure\":\n",
    "                da.attrs[\"standard_name\"] = \"Pressure\"\n",
    "            case \"air_temperature\":\n",
    "                da.attrs[\"standard_name\"] = \"Temperature\"\n",
    "            case \"altitude\":\n",
    "                da.attrs[\"standard_name\"] = \"Altitude\"\n",
    "            case \"relative_humidity\":\n",
    "                da.attrs[\"standard_name\"] = \"Relative\"\n",
    "        for string in (\"units\", \"type\"):\n",
    "            if string in var:\n",
    "                ds = ds.drop_vars(var)\n",
    "                (value,) = set(da.values)\n",
    "                attrs_var = varname if var == string else var.replace(\"_\" + string, \"\")\n",
    "                ds[attrs_var].attrs[string] = value\n",
    "    return ds\n",
    "\n",
    "\n",
    "def reorganize_dataset(ds, stations):\n",
    "    for var, da in ds.data_vars.items():\n",
    "        if np.issubdtype(da.dtype, np.bytes_):\n",
    "            ds[var].values = np.char.decode(da.values, \"utf-8\")\n",
    "\n",
    "    if stations is not None:\n",
    "        ds = ds.where(ds[\"primary_station_id\"].isin(stations), drop=True)\n",
    "\n",
    "    if not ds.sizes[\"index\"]:\n",
    "        return xr.Dataset()\n",
    "\n",
    "    datasets = []\n",
    "    for var, ds in ds.groupby(\"observed_variable\"):\n",
    "        datasets.append(_reorganize_dataset(ds))\n",
    "    ds = xr.merge(datasets)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def calculate_tropopause(ds):\n",
    "    attrs = {\"long_name\": \"WMO Lapse-Rate Tropopause\", \"units\": \"km\"}\n",
    "\n",
    "    # convert, drop, and sort\n",
    "    ds = ds.swap_dims(index=\"altitude\")[[\"air_temperature\", \"altitude\", \"pressure\"]]\n",
    "    ds = ds.where(\n",
    "        (ds[\"altitude\"].notnull() & ds[\"pressure\"].notnull()).compute(), drop=True\n",
    "    )\n",
    "    ds[\"altitude\"] = ds[\"altitude\"] * 1.0e-3\n",
    "    ds[\"pressure\"] = np.log10(ds[\"pressure\"] * 1.0e-2)\n",
    "    ds = (\n",
    "        ds.drop_duplicates(\"altitude\")\n",
    "        .swap_dims(altitude=\"pressure\")\n",
    "        .drop_duplicates(\"pressure\")\n",
    "        .swap_dims(pressure=\"altitude\")\n",
    "        .sortby(\"altitude\")\n",
    "    )\n",
    "\n",
    "    if not ds.sizes[\"altitude\"] or (ds[\"altitude\"][-1] - ds[\"altitude\"][0]) < 2:\n",
    "        # Column must be at least 2km\n",
    "        return xr.DataArray(None, attrs=attrs)\n",
    "\n",
    "    # interpolate\n",
    "    interp_altitude = np.arange(0.1, 40.1, 0.1)\n",
    "    temp = ds[\"air_temperature\"].interp(altitude=interp_altitude, method=\"cubic\")\n",
    "    temp = temp.assign_coords(pressure=10 ** ds[\"pressure\"].interp_like(temp))\n",
    "    temp = temp.dropna(\"altitude\")\n",
    "    if not temp.sizes[\"altitude\"]:\n",
    "        return xr.DataArray(None, attrs=attrs)\n",
    "\n",
    "    # compute lapse rate\n",
    "    diff_kwargs = {\"dim\": \"altitude\", \"label\": \"lower\"}\n",
    "    lapse_rate = -temp.diff(**diff_kwargs) / temp[\"altitude\"].diff(**diff_kwargs)\n",
    "    lapse_rate = lapse_rate.sel(altitude=slice(None, lapse_rate[\"altitude\"].max() - 2))\n",
    "\n",
    "    # mask and loop over valid lapse rates\n",
    "    mask = (lapse_rate <= 2) & (lapse_rate[\"pressure\"] <= 500)\n",
    "    valid_altitude = lapse_rate[\"altitude\"].where(mask.compute(), drop=True)\n",
    "    for bottom in valid_altitude:\n",
    "        temp_bottom = temp.sel(altitude=bottom)\n",
    "        temp_above = temp.sel(altitude=slice(bottom, bottom + 2)).drop_sel(\n",
    "            altitude=bottom\n",
    "        )\n",
    "        lapse_rate = (temp_bottom - temp_above) / (\n",
    "            temp_above[\"altitude\"] - temp_bottom[\"altitude\"]\n",
    "        )\n",
    "        if (lapse_rate <= 2).all():\n",
    "            return xr.DataArray(float(bottom.values), attrs=attrs)\n",
    "    return xr.DataArray(None, attrs=attrs)\n",
    "\n",
    "\n",
    "def compute_tropopause_altitude(ds, stations):\n",
    "    ds = reorganize_dataset(ds, stations).compute()\n",
    "    dataarrays = []\n",
    "    for report_id, ds_id in ds.groupby(ds[\"report_id\"]):\n",
    "        coords = {\"report_id\": (\"time\", [report_id])}\n",
    "        for var, da_coord in ds_id.data_vars.items():\n",
    "            if \"uncertainty\" in var:\n",
    "                continue\n",
    "\n",
    "            unique = set(da_coord.values)\n",
    "            if len(unique) == 1:\n",
    "                coords[var] = (\"time\", list(unique))\n",
    "        da = calculate_tropopause(ds_id)\n",
    "        dataarrays.append(da.expand_dims(\"time\").assign_coords(coords))\n",
    "    da = xr.concat(dataarrays, \"time\")\n",
    "    da = da.assign_coords(time=pd.to_datetime(da[\"report_timestamp\"]).tz_localize(None))\n",
    "    return da.sortby(\"time\").to_dataset(name=\"tropopause\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Download and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = download.download_and_transform(\n",
    "    collection_id,\n",
    "    requests,\n",
    "    chunks={\"year\": 1, \"month\": 1},\n",
    "    transform_func=compute_tropopause_altitude,\n",
    "    transform_func_kwargs={\"stations\": sorted(stations) if stations else stations},\n",
    ").compute()\n",
    "\n",
    "csv_dir_path = pathlib.Path(csv_dir)\n",
    "csv_dir_path.mkdir(exist_ok=True)\n",
    "ds.to_pandas().to_csv(csv_dir_path / \"ds_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Plot monthly timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9, 4])\n",
    "for station, da in ds[\"tropopause\"].groupby(\"primary_station_id\"):\n",
    "    da_resampled = da.resample(time=\"MS\")\n",
    "    df_mean = da_resampled.mean().to_pandas()\n",
    "    df_std = da_resampled.std().to_pandas()\n",
    "    df_mean.plot(yerr=df_std, marker=\".\", label=station)\n",
    "    df_mean.to_csv(csv_dir_path / f\"monthly_mean_lrt_{station.lower()}.csv\")\n",
    "    df_std.to_csv(csv_dir_path / f\"monthly_lrt_std_{station.lower()}.csv\")\n",
    "ax.set_title(f\"{da.attrs['long_name']} (LRT)\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(f\"Altitude [{da.attrs['units']}]\")\n",
    "ax.grid()\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Plot seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9, 4])\n",
    "for station, da in ds[\"tropopause\"].groupby(\"primary_station_id\"):\n",
    "    grouped = da.groupby(\"time.month\")\n",
    "    df_mean = grouped.mean().to_pandas()\n",
    "    df_std = grouped.std().to_pandas()\n",
    "    df_mean.plot(yerr=df_std, marker=\".\", label=station)\n",
    "    ax.set_xticks(range(1, 13), calendar.month_abbr[1:13])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(f\"Altitude [{da.attrs['units']}]\")\n",
    "    ax.set_title(\"Annual cycle of mean LRT\")\n",
    "    ax.grid()\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Plot fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[9, 5])\n",
    "for i, (station, da) in enumerate(ds[\"tropopause\"].groupby(\"primary_station_id\")):\n",
    "    if station not in [\"LIN\", \"NYA\"]:\n",
    "        continue\n",
    "\n",
    "    # Seasonal decomposition\n",
    "    df_mean = da.resample(time=\"MS\").mean().interpolate_na(\"time\").to_pandas()\n",
    "    df_anom = df_mean - df_mean.mean()\n",
    "    decomposition = statsmodels.tsa.seasonal.seasonal_decompose(\n",
    "        df_anom, model=\"additive\"\n",
    "    )\n",
    "    decomposition.trend.to_csv(csv_dir_path / f\"intra-annual_{station}.csv\")\n",
    "    decomposition.seasonal.to_csv(csv_dir_path / f\"seasonal_{station}.csv\")\n",
    "\n",
    "    # Linear fit\n",
    "    x_trend = np.arange(decomposition.trend.size)[~decomposition.trend.isnull()]\n",
    "    y_trend = decomposition.trend.dropna()\n",
    "    res = stats.linregress(x_trend, y_trend)\n",
    "    fit = pd.Series(res.intercept + res.slope * x_trend, index=y_trend.index)\n",
    "\n",
    "    # Two-sided inverse Students t-distribution\n",
    "    q = 0.05 / 2\n",
    "    deg_freedom = len(x_trend) - 2\n",
    "    ts = scipy.stats.t.ppf(q, deg_freedom)\n",
    "\n",
    "    # Plot\n",
    "    decomposition.trend.plot(color=f\"C{i}\", ls=\"-\", label=station)\n",
    "    fit.plot(color=f\"C{i}\", ls=\"--\", label=f\"linear fit for {station}\")\n",
    "\n",
    "    # Text\n",
    "    print(f\"{station}:\")\n",
    "    pad = 17\n",
    "    print(f\"{'Equation':>{pad}}: {res.slope:+.4f}x {res.intercept:+.4f}\")\n",
    "    print(f\"{'R^2':>{pad}}: {res.rvalue**2:.2f}\")\n",
    "    print(\n",
    "        f\"{'p-value':>{pad}}: \" + \"< 0.001\"\n",
    "        if res.pvalue < 0.001\n",
    "        else f\"{res.pvalue:.4f}\"\n",
    "    )\n",
    "    print(f\"{'slope (95%)':>{pad}}: {res.slope:+.4f} ± {abs(ts * res.stderr):.4f}\")\n",
    "    print(\n",
    "        f\"{'intercept (95%)':>{pad}}: {res.intercept:+.4f} ± {abs(ts * res.intercept_stderr):.4f}\"\n",
    "    )\n",
    "\n",
    "# Plot settings\n",
    "ax.set_title(\"Linear trends\")\n",
    "ax.set_ylabel(f\"Altitude anomaly [{da.attrs['units']}]\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.grid()\n",
    "_ = ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
