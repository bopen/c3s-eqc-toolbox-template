{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ERA5 vs GRUAN\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cdsapi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import download\n",
    "\n",
    "os.environ[\"CDSAPI_RC\"] = os.path.expanduser(\"~/ciardini_virginia/.cdsapirc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time period\n",
    "start = \"2006-05\"\n",
    "stop = \"2020-03\"\n",
    "\n",
    "# Regions\n",
    "stations = {\n",
    "    \"NYA\": {\"latitude\": 78.92, \"longitude\": 11.93},\n",
    "}\n",
    "assert isinstance(stations, dict)\n",
    "\n",
    "# Pressure levels\n",
    "levels = [\n",
    "    \"100\",\n",
    "    \"125\",\n",
    "    \"150\",\n",
    "    \"175\",\n",
    "    \"200\",\n",
    "    \"225\",\n",
    "    \"250\",\n",
    "    \"300\",\n",
    "    \"350\",\n",
    "    \"400\",\n",
    "    \"450\",\n",
    "    \"500\",\n",
    "    \"550\",\n",
    "    \"600\",\n",
    "    \"650\",\n",
    "    \"700\",\n",
    "    \"750\",\n",
    "    \"775\",\n",
    "    \"800\",\n",
    "    \"825\",\n",
    "    \"850\",\n",
    "    \"875\",\n",
    "    \"900\",\n",
    "    \"925\",\n",
    "    \"950\",\n",
    "    \"975\",\n",
    "    \"1000\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Define requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUAN\n",
    "collection_id_gruan = \"insitu-observations-gruan-reference-network\"\n",
    "request_gruan = {\n",
    "    \"version\": \"1_0_0\",\n",
    "    \"variable\": [\n",
    "        \"air_temperature\",\n",
    "        \"relative_humidity\",\n",
    "        \"air_pressure\",\n",
    "        \"altitude\",\n",
    "        \"eastward_wind_speed\",\n",
    "        \"northward_wind_speed\",\n",
    "    ],\n",
    "    \"data_format\": \"netcdf\",\n",
    "}\n",
    "\n",
    "# ERA5\n",
    "collection_id_era5 = \"reanalysis-era5-pressure-levels-monthly-means\"\n",
    "request_era5 = {\n",
    "    \"product_type\": \"monthly_averaged_reanalysis\",\n",
    "    \"variable\": [\n",
    "        \"temperature\",\n",
    "        \"u_component_of_wind\",\n",
    "        \"v_component_of_wind\",\n",
    "        \"relative_humidity\",\n",
    "    ],\n",
    "    \"pressure_level\": levels,\n",
    "    \"time\": \"00:00\",\n",
    "    \"data_format\": \"grib\",\n",
    "    \"download_format\": \"unarchived\",\n",
    "}\n",
    "\n",
    "# Build requests\n",
    "client = cdsapi.Client()\n",
    "requests_gruan = []\n",
    "requests_era5 = []\n",
    "for date in pd.date_range(start, stop, freq=\"1MS\"):\n",
    "    # GRUAN\n",
    "    time_request = {\"year\": date.strftime(\"%Y\"), \"month\": date.strftime(\"%m\")}\n",
    "    time_request[\"day\"] = client.client.apply_constraints(\n",
    "        collection_id_gruan, request_gruan | time_request\n",
    "    )[\"day\"]\n",
    "    if time_request[\"day\"]:\n",
    "        requests_gruan.append(request_gruan | time_request)\n",
    "    # ERA5\n",
    "    requests_era5.append(request_era5 | time_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reorganize_dataset(ds):\n",
    "    # Rename\n",
    "    (varname,) = set(ds[\"observed_variable\"].values)\n",
    "    ds = ds.rename(observation_value=str(varname)).drop_vars(\"observed_variable\")\n",
    "    ds = ds.rename(\n",
    "        {\n",
    "            var: \"_\".join([varname, var.replace(\"_value\", \"\")])\n",
    "            for var in ds.data_vars\n",
    "            if var.startswith(\"uncertainty\")\n",
    "        }\n",
    "    )\n",
    "    # Update attrs\n",
    "    for var, da in ds.data_vars.items():\n",
    "        da.attrs[\"long_name\"] = var.replace(\"_\", \" \").title()\n",
    "        for string in (\"units\", \"type\"):\n",
    "            if string in var:\n",
    "                ds = ds.drop_vars(var)\n",
    "                (value,) = set(da.values)\n",
    "                attrs_var = varname if var == string else var.replace(\"_\" + string, \"\")\n",
    "                ds[attrs_var].attrs[string] = value\n",
    "    return ds\n",
    "\n",
    "\n",
    "def reorganize_dataset(ds):\n",
    "    for var, da in ds.data_vars.items():\n",
    "        if np.issubdtype(da.dtype, np.bytes_):\n",
    "            ds[var].values = np.char.decode(da.values, \"utf-8\")\n",
    "\n",
    "    if not ds.sizes[\"index\"]:\n",
    "        return ds\n",
    "\n",
    "    datasets = []\n",
    "    for var, ds in ds.groupby(\"observed_variable\"):\n",
    "        datasets.append(_reorganize_dataset(ds))\n",
    "    with xr.set_options(use_new_combine_kwarg_defaults=True):\n",
    "        return xr.merge(datasets)\n",
    "\n",
    "\n",
    "def compute_interpolated_insitu_profiles(ds, levels, variables):\n",
    "    ds = reorganize_dataset(ds)\n",
    "\n",
    "    # Add time index\n",
    "    ds[\"time\"] = (\"index\", pd.to_datetime(ds[\"report_timestamp\"]).values)\n",
    "\n",
    "    # Variables to retain\n",
    "    profiles = []\n",
    "    for station, ds_station in ds.groupby(\"primary_station_id\"):\n",
    "        for time, profile in ds_station.groupby(\"time\"):\n",
    "            # Organizza il profilo per altitudine\n",
    "            profile = profile.swap_dims(index=\"altitude\")[variables]\n",
    "            profile = profile.sortby(\"altitude\")\n",
    "            profile = profile.dropna(\"altitude\", how=\"any\", subset=variables)\n",
    "            profile = profile.drop_duplicates(\"altitude\")\n",
    "\n",
    "            # Quality check\n",
    "            if (\n",
    "                not profile.sizes[\"altitude\"]\n",
    "                or (profile[\"altitude\"].diff(\"altitude\") > 2_000).any()\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # Interpolate\n",
    "            if \"pressure\" not in profile:\n",
    "                profile[\"pressure\"] = 1013.25 * np.exp(-profile[\"altitude\"] / 8434.5)\n",
    "            profile = profile.swap_dims(altitude=\"pressure\").drop_duplicates(\"pressure\")\n",
    "            try:\n",
    "                profile = profile.interp(pressure=levels)\n",
    "            except Exception:\n",
    "                print(profile)\n",
    "                raise\n",
    "            profile[\"pressure\"].attrs.update({\"long_name\": \"Pressure\", \"units\": \"hPa\"})\n",
    "\n",
    "            # Append\n",
    "            profile = profile.expand_dims(time=[time])\n",
    "            profile = profile.assign_coords(station=(\"time\", [station]))\n",
    "            profiles.append(profile)\n",
    "    return xr.concat(profiles, dim=\"time\")\n",
    "\n",
    "\n",
    "def select_nearest_station(ds, latitude, longitude):\n",
    "    return ds.sel(latitude=latitude, longitude=longitude, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## GRUAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gruan = download.download_and_transform(\n",
    "    collection_id_gruan,\n",
    "    requests_gruan,\n",
    "    chunks={\"year\": 1, \"month\": 1},\n",
    "    transform_func=compute_interpolated_insitu_profiles,\n",
    "    transform_func_kwargs={\n",
    "        \"levels\": sorted(map(float, levels)),\n",
    "        \"variables\": sorted(\n",
    "            [\n",
    "                \"pressure\" if variable == \"air_pressure\" else variable\n",
    "                for variable in request_gruan[\"variable\"]\n",
    "            ]\n",
    "        ),\n",
    "    },\n",
    "    cached_open_mfdataset_kwargs={\"concat_dim\": \"time\", \"combine\": \"nested\"},\n",
    ")\n",
    "\n",
    "if stations is not None:\n",
    "    ds_gruan = ds_gruan.where(\n",
    "        ds_gruan[\"station\"].isin(sorted(stations)).compute(), drop=True\n",
    "    )\n",
    "ds_gruan = ds_gruan.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## ERA5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for station, transform_func_kwargs in stations.items():\n",
    "    print(f\"{station=}\")\n",
    "    ds = download.download_and_transform(\n",
    "        collection_id_era5,\n",
    "        requests_era5,\n",
    "        transform_func=select_nearest_station,\n",
    "        transform_func_kwargs=transform_func_kwargs,\n",
    "    )\n",
    "    datasets.append(ds.expand_dims(station=[station]))\n",
    "ds_era5 = xr.concat(datasets, \"station\").compute()\n",
    "\n",
    "# Convert plev to hPa (mb)\n",
    "ds_era5[\"plev\"] = ds_era5[\"plev\"] / 100\n",
    "ds_era5[\"plev\"].attrs.update({\"long_name\": \"Level\", \"units\": \"hPa\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
