{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Seasonal forecast monthly averages of ocean variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import diagnostics, download, plot\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "start = \"1993-05\"\n",
    "stop = \"2025-01\"\n",
    "freq = \"12MS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id_reanalysis = \"reanalysis-oras5\"\n",
    "collection_id_seasonal = \"seasonal-monthly-ocean\"\n",
    "\n",
    "requests_reanalysis = []\n",
    "requests_seasonal = []\n",
    "\n",
    "for date in pd.date_range(start, stop, freq=freq):\n",
    "    time_request = {\"year\": date.strftime(\"%Y\"), \"month\": date.strftime(\"%m\")}\n",
    "    requests_reanalysis.append(\n",
    "        {\n",
    "            \"product_type\": [\"operational\" if date.year > 2014 else \"consolidated\"],\n",
    "            \"vertical_resolution\": \"single_level\",\n",
    "            \"variable\": [\"ocean_heat_content_for_the_upper_300m\"],\n",
    "        }\n",
    "        | time_request\n",
    "    )\n",
    "\n",
    "    if pd.to_datetime(\"2019-01\") <= date <= pd.to_datetime(\"2023-03\"):\n",
    "        continue\n",
    "    requests_seasonal.append(\n",
    "        {\n",
    "            \"originating_centre\": \"meteo_france\",\n",
    "            \"system\": \"8\",\n",
    "            \"variable\": [\"depth_average_potential_temperature_of_upper_300m\"],\n",
    "            \"forecast_type\": [\"forecast\" if date.year > 2018 else \"hindcast\"],\n",
    "        }\n",
    "        | time_request\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounds(ds):\n",
    "    # From https://github.com/COSIMA/ocean-regrid/blob/master/nemo_grid.py\n",
    "    dg = xr.open_dataset(\n",
    "        (\n",
    "            \"https://icdc.cen.uni-hamburg.de/thredds/dodsC/ftpthredds/\"\n",
    "            \"EASYInit/oras5/ORCA025/mesh/mesh_mask.nc\"\n",
    "        ),\n",
    "        chunks={},\n",
    "    ).isel(t=0, z=0)\n",
    "\n",
    "    # These are the top righ-hand corner of t cells.\n",
    "    glamf = dg.glamf\n",
    "    gphif = dg.gphif\n",
    "\n",
    "    # Extend south so that Southern most cells can have bottom corners.\n",
    "    gphif_new = np.ndarray((gphif.shape[0] + 1, gphif.shape[1] + 1))\n",
    "    gphif_new[1:, 1:] = gphif[:]\n",
    "    gphif_new[0, 1:] = gphif[0, :] - abs(gphif[1, :] - gphif[0, :])\n",
    "\n",
    "    glamf_new = np.ndarray((glamf.shape[0] + 1, glamf.shape[1] + 1))\n",
    "    glamf_new[1:, 1:] = glamf[:]\n",
    "    glamf_new[0, 1:] = glamf[0, :]\n",
    "\n",
    "    # Repeat first longitude so that Western most cells have left corners.\n",
    "    gphif_new[:, 0] = gphif_new[:, -1]\n",
    "    glamf_new[:, 0] = glamf_new[:, -1]\n",
    "\n",
    "    gphif = gphif_new\n",
    "    glamf = glamf_new\n",
    "\n",
    "    # Corners of t points. Index 0 is bottom left and then\n",
    "    # anti-clockwise.\n",
    "    clon = np.empty((dg.tmask.shape[0], dg.tmask.shape[1], 4))\n",
    "    clon[:] = np.nan\n",
    "    clon[:, :, 0] = glamf[0:-1, 0:-1]\n",
    "    clon[:, :, 1] = glamf[0:-1, 1:]\n",
    "    clon[:, :, 2] = glamf[1:, 1:]\n",
    "    clon[:, :, 3] = glamf[1:, 0:-1]\n",
    "    assert not np.isnan(np.sum(clon))\n",
    "\n",
    "    clat = np.empty((dg.tmask.shape[0], dg.tmask.shape[1], 4))\n",
    "    clat[:] = np.nan\n",
    "    clat[:, :, 0] = gphif[0:-1, 0:-1]\n",
    "    clat[:, :, 1] = gphif[0:-1, 1:]\n",
    "    clat[:, :, 2] = gphif[1:, 1:]\n",
    "    clat[:, :, 3] = gphif[1:, 0:-1]\n",
    "    assert not np.isnan(np.sum(clat))\n",
    "\n",
    "    ds[\"latitude\"].attrs[\"bounds\"] = \"latitude_bounds\"\n",
    "    ds[\"longitude\"].attrs[\"bounds\"] = \"longitude_bounds\"\n",
    "    return ds.assign_coords(\n",
    "        latitude_bounds=([\"y\", \"x\", \"bound\"], clat),\n",
    "        longitude_bounds=([\"y\", \"x\", \"bound\"], clon),\n",
    "    )\n",
    "\n",
    "\n",
    "# Seasonal\n",
    "def preprocess_seasonal(ds):\n",
    "    # TODO: How to combine? Use first leadtime only for now\n",
    "    ds = ds.set_coords([var for var, da in ds.data_vars.items() if \"bnds\" in da.dims])\n",
    "    ds[\"realization\"] = ds[\"realization\"].str.replace(\" \", \"\").astype(str)\n",
    "    return ds.expand_dims([\"realization\", \"reftime\"])\n",
    "\n",
    "\n",
    "def regrid_reanalysis(ds, grid_request, **xesmf_kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(  # Suppress decode_timedelta warning\n",
    "            \"ignore\", message=\".*decode_timedelta.*\"\n",
    "        )\n",
    "        ds_seasonal = download.download_and_transform(\n",
    "            *grid_request,\n",
    "            preprocess=preprocess_seasonal,\n",
    "        )\n",
    "    mask_out = (\n",
    "        ds_seasonal[\"thetaot300\"]\n",
    "        .isel(\n",
    "            {dim: 0 for dim in (\"realization\", \"forecast_reference_time\", \"leadtime\")}\n",
    "        )\n",
    "        .reset_coords(drop=True)\n",
    "        .notnull()\n",
    "    )\n",
    "    grid_out = ds_seasonal.cf[[\"latitude\", \"longitude\"]].assign_coords(mask=mask_out)\n",
    "\n",
    "    mask_in = ds[\"sohtc300\"].isel(time=0).reset_coords(drop=True).notnull()\n",
    "    ds = add_bounds(ds).assign_coords(mask=mask_in)\n",
    "    return diagnostics.regrid(ds, grid_out, **xesmf_kwargs)\n",
    "\n",
    "\n",
    "def compute_detrended_anomalies(ds, grid_request, **xesmf_kwargs):\n",
    "    if grid_request is not None:\n",
    "        ds = regrid_reanalysis(ds, grid_request, **xesmf_kwargs)\n",
    "    else:\n",
    "        assert not xesmf_kwargs\n",
    "\n",
    "    (da,) = ds.data_vars.values()\n",
    "    name = da.name\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        # 2.0: Calculating the ensemble mean for each lead time, and year\n",
    "        if \"realization\" in da.dims:\n",
    "            ensemble_mean = da.mean(\"realization\")\n",
    "        else:\n",
    "            ensemble_mean = da\n",
    "\n",
    "        # 3.0: Calculating a trend for each lead time based on 2.0: the ensemble means.\n",
    "        (time_dim,) = set(ds.dims) & {\"time\", \"forecast_reference_time\"}\n",
    "        trend = xr.polyval(\n",
    "            ensemble_mean[time_dim],\n",
    "            ensemble_mean.polyfit(time_dim, deg=1).polyfit_coefficients,\n",
    "        )\n",
    "\n",
    "        # 3.1: Subtracting the lead-time specific trend for each ensemble member, year, and lead time\n",
    "        detrended = da - trend\n",
    "\n",
    "        # 4.0: Calculating the climatology based on 2.0: the ensemble mean for each lead time\n",
    "        climatology = ensemble_mean.mean(time_dim)\n",
    "\n",
    "        # 4.1: Subtracting the lead-time specific climatology for each ensemble member, year, and lead time\n",
    "        da = detrended - climatology\n",
    "\n",
    "    da.encoding[\"chunksizes\"] = tuple(\n",
    "        1 if dim in (\"realization\", \"leadtime\") else size\n",
    "        for dim, size in da.sizes.items()\n",
    "    )\n",
    "    return da.to_dataset(name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Download and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal\n",
    "ds_seasonal = download.download_and_transform(\n",
    "    collection_id_seasonal,\n",
    "    requests_seasonal,\n",
    "    preprocess=preprocess_seasonal,\n",
    "    drop_variables=\"hcrs\",\n",
    "    transform_func=compute_detrended_anomalies,\n",
    "    transform_func_kwargs={\"grid_request\": None},\n",
    "    transform_chunks=False,\n",
    ")\n",
    "ds_seasonal = ds_seasonal.isel(leadtime=slice(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reanalysis\n",
    "ds_reanalysis = download.download_and_transform(\n",
    "    collection_id_reanalysis,\n",
    "    requests_reanalysis,\n",
    "    transform_func=compute_detrended_anomalies,\n",
    "    drop_variables=\"time_counter_bnds\",\n",
    "    transform_func_kwargs={\n",
    "        \"grid_request\": (collection_id_seasonal, requests_seasonal[0]),\n",
    "        \"method\": \"conservative_normed\",\n",
    "        \"periodic\": True,\n",
    "        \"ignore_degenerate\": True,\n",
    "    },\n",
    "    transform_chunks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Quick and dirty plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "(da,) = ds_reanalysis.data_vars.values()\n",
    "_ = plot.projected_map(da.mean(\"time\", keep_attrs=True))\n",
    "plt.show()\n",
    "\n",
    "diagnostics.spatial_weighted_mean(da).plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Seasonal Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(da,) = ds_seasonal.mean(\"realization\").data_vars.values()\n",
    "_ = plot.projected_map(\n",
    "    da.mean(\"forecast_reference_time\", keep_attrs=True), col=\"leadtime\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "diagnostics.spatial_weighted_mean(da).plot(hue=\"leadtime\", x=\"time\")\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
