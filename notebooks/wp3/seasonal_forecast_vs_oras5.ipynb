{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Seasonal forecast monthly averages of ocean variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from c3s_eqc_automatic_quality_control import diagnostics, download, plot\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "start = \"1993-05\"\n",
    "stop = \"2025-01\"\n",
    "freq = \"12MS\"\n",
    "leadtimes = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Select realizations for ensemble\n",
    "realizations = slice(None, 25)\n",
    "\n",
    "# Whether to detrend anomalies or not\n",
    "detrend = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Define requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id_reanalysis = \"reanalysis-oras5\"\n",
    "collection_id_seasonal = \"seasonal-monthly-ocean\"\n",
    "\n",
    "requests_reanalysis = {leadtime: [] for leadtime in leadtimes}\n",
    "requests_seasonal = []\n",
    "for date in pd.date_range(start, stop, freq=freq):\n",
    "    requests_seasonal.append(\n",
    "        {\n",
    "            \"originating_centre\": \"meteo_france\",\n",
    "            \"system\": \"9\",\n",
    "            \"variable\": [\"depth_average_potential_temperature_of_upper_300m\"],\n",
    "            \"forecast_type\": [\"hindcast\"],\n",
    "            \"year\": date.strftime(\"%Y\"),\n",
    "            \"month\": date.strftime(\"%m\"),\n",
    "        }\n",
    "    )\n",
    "    for leadtime in leadtimes:\n",
    "        date_reanalysis = date + pd.DateOffset(months=leadtime)\n",
    "        requests_reanalysis[leadtime].append(\n",
    "            {\n",
    "                \"product_type\": [\"operational\" if date.year > 2014 else \"consolidated\"],\n",
    "                \"vertical_resolution\": \"single_level\",\n",
    "                \"variable\": [\"ocean_heat_content_for_the_upper_300m\"],\n",
    "                \"year\": date_reanalysis.strftime(\"%Y\"),\n",
    "                \"month\": date_reanalysis.strftime(\"%m\"),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Functions to cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounds(ds):\n",
    "    # From https://github.com/COSIMA/ocean-regrid/blob/master/nemo_grid.py\n",
    "    dg = xr.open_dataset(\n",
    "        (\n",
    "            \"https://icdc.cen.uni-hamburg.de/thredds/dodsC/ftpthredds/\"\n",
    "            \"EASYInit/oras5/ORCA025/mesh/mesh_mask.nc\"\n",
    "        ),\n",
    "        chunks={},\n",
    "    ).isel(t=0, z=0)\n",
    "\n",
    "    # These are the top righ-hand corner of t cells.\n",
    "    glamf = dg.glamf\n",
    "    gphif = dg.gphif\n",
    "\n",
    "    # Extend south so that Southern most cells can have bottom corners.\n",
    "    gphif_new = np.ndarray((gphif.shape[0] + 1, gphif.shape[1] + 1))\n",
    "    gphif_new[1:, 1:] = gphif[:]\n",
    "    gphif_new[0, 1:] = gphif[0, :] - abs(gphif[1, :] - gphif[0, :])\n",
    "\n",
    "    glamf_new = np.ndarray((glamf.shape[0] + 1, glamf.shape[1] + 1))\n",
    "    glamf_new[1:, 1:] = glamf[:]\n",
    "    glamf_new[0, 1:] = glamf[0, :]\n",
    "\n",
    "    # Repeat first longitude so that Western most cells have left corners.\n",
    "    gphif_new[:, 0] = gphif_new[:, -1]\n",
    "    glamf_new[:, 0] = glamf_new[:, -1]\n",
    "\n",
    "    gphif = gphif_new\n",
    "    glamf = glamf_new\n",
    "\n",
    "    # Corners of t points. Index 0 is bottom left and then\n",
    "    # anti-clockwise.\n",
    "    clon = np.empty((dg.tmask.shape[0], dg.tmask.shape[1], 4))\n",
    "    clon[:] = np.nan\n",
    "    clon[:, :, 0] = glamf[0:-1, 0:-1]\n",
    "    clon[:, :, 1] = glamf[0:-1, 1:]\n",
    "    clon[:, :, 2] = glamf[1:, 1:]\n",
    "    clon[:, :, 3] = glamf[1:, 0:-1]\n",
    "    assert not np.isnan(np.sum(clon))\n",
    "\n",
    "    clat = np.empty((dg.tmask.shape[0], dg.tmask.shape[1], 4))\n",
    "    clat[:] = np.nan\n",
    "    clat[:, :, 0] = gphif[0:-1, 0:-1]\n",
    "    clat[:, :, 1] = gphif[0:-1, 1:]\n",
    "    clat[:, :, 2] = gphif[1:, 1:]\n",
    "    clat[:, :, 3] = gphif[1:, 0:-1]\n",
    "    assert not np.isnan(np.sum(clat))\n",
    "\n",
    "    ds[\"latitude\"].attrs[\"bounds\"] = \"latitude_bounds\"\n",
    "    ds[\"longitude\"].attrs[\"bounds\"] = \"longitude_bounds\"\n",
    "    return ds.assign_coords(\n",
    "        latitude_bounds=([\"y\", \"x\", \"bound\"], clat),\n",
    "        longitude_bounds=([\"y\", \"x\", \"bound\"], clon),\n",
    "    )\n",
    "\n",
    "\n",
    "# Seasonal\n",
    "def preprocess_seasonal(ds):\n",
    "    # TODO: How to combine? Use first leadtime only for now\n",
    "    ds = ds.set_coords([var for var, da in ds.data_vars.items() if \"bnds\" in da.dims])\n",
    "    ds[\"realization\"] = ds[\"realization\"].str.replace(\" \", \"\").astype(str)\n",
    "    return ds.expand_dims([\"realization\", \"reftime\"])\n",
    "\n",
    "\n",
    "def regrid_reanalysis(ds, grid_request, **xesmf_kwargs):\n",
    "    ds_seasonal = download.download_and_transform(\n",
    "        *grid_request,\n",
    "        preprocess=preprocess_seasonal,\n",
    "        invalidate_cache=False,\n",
    "    )\n",
    "    mask_out = (\n",
    "        ds_seasonal[\"thetaot300\"]\n",
    "        .isel(\n",
    "            {dim: 0 for dim in (\"realization\", \"forecast_reference_time\", \"leadtime\")}\n",
    "        )\n",
    "        .reset_coords(drop=True)\n",
    "        .notnull()\n",
    "    )\n",
    "    grid_out = ds_seasonal.cf[[\"latitude\", \"longitude\"]].assign_coords(mask=mask_out)\n",
    "\n",
    "    mask_in = ds[\"sohtc300\"].isel(time=0).reset_coords(drop=True).notnull()\n",
    "    ds = add_bounds(ds).assign_coords(mask=mask_in)\n",
    "    return diagnostics.regrid(ds, grid_out, **xesmf_kwargs)\n",
    "\n",
    "\n",
    "def compute_anomalies(ds, realizations, grid_request, detrend, **xesmf_kwargs):\n",
    "    if realizations is not None:\n",
    "        ds = ds.isel(realization=realizations)\n",
    "\n",
    "    if grid_request is not None:\n",
    "        ds = regrid_reanalysis(ds, grid_request, **xesmf_kwargs)\n",
    "    else:\n",
    "        assert not xesmf_kwargs\n",
    "\n",
    "    ((name, da),) = ds.data_vars.items()\n",
    "    (time_dim,) = set(da.dims) & {\"time\", \"forecast_reference_time\"}\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        da = da - da.mean(set(da.dims) & {\"realization\", time_dim})\n",
    "        if detrend:\n",
    "            trend = xr.polyval(\n",
    "                da[time_dim], da.polyfit(time_dim, deg=1).polyfit_coefficients\n",
    "            )\n",
    "            da = da - trend\n",
    "\n",
    "    da.encoding[\"chunksizes\"] = tuple(\n",
    "        1 if dim in (\"realization\", \"leadtime\") else size\n",
    "        for dim, size in da.sizes.items()\n",
    "    )\n",
    "    return da.to_dataset(name=name)\n",
    "\n",
    "\n",
    "def reindex(ds):\n",
    "    # Reindex using year/month (shift months)\n",
    "    ds = ds.assign_coords(\n",
    "        year=(\"time\", ds[\"time\"].dt.year.astype(int).values),\n",
    "        month=(\"time\", ds[\"time\"].dt.month.astype(int).values),\n",
    "    )\n",
    "    ds = ds.set_index(time=(\"year\", \"month\")).unstack(\"time\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Download and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal\n",
    "ds_seasonal = download.download_and_transform(\n",
    "    collection_id_seasonal,\n",
    "    requests_seasonal,\n",
    "    preprocess=preprocess_seasonal,\n",
    "    drop_variables=\"hcrs\",\n",
    "    transform_func=compute_anomalies,\n",
    "    transform_func_kwargs={\n",
    "        \"realizations\": realizations,\n",
    "        \"detrend\": detrend,\n",
    "        \"grid_request\": None,\n",
    "    },\n",
    "    transform_chunks=False,\n",
    ")\n",
    "datasets = []\n",
    "for leadtime, ds in ds_seasonal.isel(leadtime=leadtimes).groupby(\"leadtime\"):\n",
    "    ds = ds.squeeze(\"leadtime\").swap_dims(forecast_reference_time=\"time\")\n",
    "    ds = reindex(ds)\n",
    "    ds[\"leadtime\"] = ds[\"leadtime\"].expand_dims(\"month\")\n",
    "    datasets.append(ds)\n",
    "ds_seasonal = xr.combine_by_coords(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reanalysis\n",
    "datasets = []\n",
    "for leadtime, requests in requests_reanalysis.items():\n",
    "    ds = download.download_and_transform(\n",
    "        collection_id_reanalysis,\n",
    "        requests,\n",
    "        transform_func=compute_anomalies,\n",
    "        drop_variables=\"time_counter_bnds\",\n",
    "        transform_func_kwargs={\n",
    "            \"realizations\": None,\n",
    "            \"detrend\": detrend,\n",
    "            \"grid_request\": (collection_id_seasonal, requests_seasonal[0]),\n",
    "            \"method\": \"conservative_normed\",\n",
    "            \"periodic\": True,\n",
    "            \"ignore_degenerate\": True,\n",
    "        },\n",
    "        transform_chunks=False,\n",
    "    )\n",
    "    datasets.append(reindex(ds))\n",
    "ds_reanalysis = xr.combine_by_coords(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Quick and dirty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"Reanalysis\": ds_reanalysis, \"Seasonal Forecast\": ds_seasonal}\n",
    "for product, ds in datasets.items():\n",
    "    (da,) = ds.data_vars.values()\n",
    "    if \"realization\" in da.dims:\n",
    "        da = da.mean(\"realization\", keep_attrs=True)\n",
    "\n",
    "    _ = plot.projected_map(da.mean(\"year\", keep_attrs=True), col=\"month\", col_wrap=3)\n",
    "    plt.suptitle(f\"{product = }\")\n",
    "    plt.show()\n",
    "\n",
    "    diagnostics.spatial_weighted_mean(da).plot(hue=\"month\")\n",
    "    plt.suptitle(f\"{product = }\")\n",
    "    plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
